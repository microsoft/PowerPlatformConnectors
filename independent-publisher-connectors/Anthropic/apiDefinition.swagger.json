{
  "swagger": "2.0",
  "info": {
    "title": "Anthropic",
    "description": "Claude is a large language model (LLM) built by Anthropic. It's trained to be a helpful assistant in a conversational tone.",
    "version": "1.1.0",
    "contact": {
      "name": "Troy Taylor",
      "url": "https://www.hitachisolutions.com",
      "email": "ttaylor@hitachisolutions.com"
    }
  },
  "host": "api.anthropic.com",
  "basePath": "/",
  "schemes": [
    "https"
  ],
  "consumes": [
    "application/json"
  ],
  "produces": [
    "application/json"
  ],
  "paths": {
    "/v1/complete": {
      "post": {
        "responses": {
          "200": {
            "description": "default",
            "schema": {
              "type": "object",
              "properties": {
                "completion": {
                  "type": "string",
                  "description": "The completion.",
                  "title": "Completion"
                },
                "stop": {
                  "type": "string",
                  "description": "The stop.",
                  "title": "Stop"
                },
                "stop_reason": {
                  "type": "string",
                  "description": "The stop reason.",
                  "title": "Stop Reason"
                },
                "truncated": {
                  "type": "boolean",
                  "description": "Whether truncated.",
                  "title": "Truncated"
                },
                "log_id": {
                  "type": "string",
                  "description": "The log identifier.",
                  "title": "Log ID"
                },
                "model": {
                  "type": "string",
                  "description": "The model.",
                  "title": "Model"
                },
                "exception": {
                  "type": "string",
                  "description": "The exception.",
                  "title": "Exception"
                }
              }
            }
          }
        },
        "summary": "Ask Claude",
        "description": "Send a prompt to Claude for completion.",
        "operationId": "ClaudePost",
        "parameters": [
          {
            "name": "body",
            "in": "body",
            "required": false,
            "schema": {
              "type": "object",
              "properties": {
                "prompt": {
                  "type": "string",
                  "description": "The prompt you want Claude to complete.",
                  "title": "Prompt"
                },
                "model": {
                  "type": "string",
                  "description": "The version of Claude answering your request.",
                  "title": "Model",
                  "default": "claude-2",
                  "enum": [
                    "claude-2",
                    "claude-instant-1"
                  ]
                },
                "max_tokens_to_sample": {
                  "type": "integer",
                  "format": "int32",
                  "description": "A maximum number of tokens to generate before stopping.",
                  "title": "Max Tokens To Sample"
                },
                "stop_sequences": {
                  "type": "array",
                  "items": {
                    "type": "string"
                  },
                  "description": "A list of strings upon which to stop generating. You probably want [\"\n\nHuman:\"], as that's the cue for the next turn in the dialog agent.",
                  "title": "Stop Sequences"
                },
                "stream": {
                  "type": "boolean",
                  "description": "Whether to incrementally stream the response using SSE.",
                  "title": "Stream"
                },
                "temperature": {
                  "type": "number",
                  "format": "float",
                  "description": "Amount of randomness injected into the response. Ranges from 0 to 1. Use temp closer to 0 for analytical / multiple choice, and temp closer to 1 for creative and generative tasks.",
                  "title": "Temperature"
                },
                "top_k": {
                  "type": "integer",
                  "format": "int32",
                  "description": "Only sample from the top K options for each subsequent token.",
                  "title": "Top K"
                },
                "top_p": {
                  "type": "number",
                  "format": "float",
                  "description": "Does nucleus sampling, in which we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p.",
                  "title": "Top P"
                }
              },
              "required": [
                "prompt",
                "max_tokens_to_sample"
              ]
            }
          }
        ]
      }
    }
  },
  "definitions": {},
  "parameters": {},
  "responses": {},
  "securityDefinitions": {
    "API Key": {
      "type": "apiKey",
      "in": "header",
      "name": "X-API-Key"
    }
  },
  "security": [
    {
      "API Key": []
    }
  ],
  "tags": [],
  "x-ms-connector-metadata": [
    {
      "propertyName": "Website",
      "propertyValue": "https://www.anthropic.com/"
    },
    {
      "propertyName": "Privacy policy",
      "propertyValue": "https://console.anthropic.com/legal/privacy"
    },
    {
      "propertyName": "Categories",
      "propertyValue": "AI"
    }
  ]
}
