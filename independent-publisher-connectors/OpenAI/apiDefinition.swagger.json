{
  "swagger": "2.0",
  "info": {
    "title": "OpenAI",
    "description": "Connect to the OpenAI API and use the Power of GPT3, API key must be entered as \"Bearer YOUR_API_KEY\"",
    "version": "1.0",
    "contact": {
      "name": "Robin Rosengr√ºn",
      "url": "https://linktr.ee/r2power",
      "email": "robin@r2power.de"
      }
  },
  "host": "api.openai.com",
  "basePath": "/",
  "schemes": [
    "https"
  ],
  "consumes": [],
  "produces": [],
  "paths": {
    "/v1/chat/completions": {
      "post": {
        "responses": {
          "200": {
            "description": "default",
            "schema": {
              "type": "object",
              "properties": {
                "id": {
                  "type": "string",
                  "description": "id"
                },
                "object": {
                  "type": "string",
                  "description": "object"
                },
                "created": {
                  "type": "integer",
                  "format": "int32",
                  "description": "created"
                },
                "choices": {
                  "type": "array",
                  "items": {
                    "type": "object",
                    "properties": {
                      "index": {
                        "type": "integer",
                        "format": "int32",
                        "description": "index"
                      },
                      "message": {
                        "type": "object",
                        "properties": {
                          "role": {
                            "type": "string",
                            "description": "role"
                          },
                          "content": {
                            "type": "string",
                            "description": "content"
                          }
                        },
                        "description": "message"
                      },
                      "finish_reason": {
                        "type": "string",
                        "description": "finish_reason"
                      }
                    }
                  },
                  "description": "choices"
                },
                "usage": {
                  "type": "object",
                  "properties": {
                    "prompt_tokens": {
                      "type": "integer",
                      "format": "int32",
                      "description": "prompt_tokens"
                    },
                    "completion_tokens": {
                      "type": "integer",
                      "format": "int32",
                      "description": "completion_tokens"
                    },
                    "total_tokens": {
                      "type": "integer",
                      "format": "int32",
                      "description": "total_tokens"
                    }
                  },
                  "description": "usage"
                }
              }
            }
          }
        },
        "summary": "Chat Completion",
        "operationId": "ChatCompletion",
        "description": "Use models like ChatGPT and GPT4 to hold a conversation",
        "x-ms-visibility": "important",
        "parameters": [
          {
            "name": "body",
            "in": "body",
            "required": false,
            "schema": {
              "type": "object",
              "properties": {
                "model": {
                  "type": "string",
                  "description": "The used model, choose between gpt-3.5-turbo, gpt-4 and others"
                },
                "messages": {
                  "type": "array",
                  "items": {
                    "type": "object",
                    "properties": {
                      "role": {
                        "type": "string",
                        "description": "The role of the author of this message. One of system, user, or assistant."
                      },
                      "content": {
                        "type": "string",
                        "description": "The contents of the message."
                      }
                    },
                    "required": [
                      "role",
                      "content"
                    ]
                  },
                  "description": "messages"
                },
                "n": {
                  "type": "integer",
                  "format": "int32",
                  "description": "How many completions to generate for each prompt",
                  "default": 1
                },
                "temperature": {
                  "type": "number",
                  "format": "float",
                  "description": "Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer. Use this OR top p",
                  "title": "temperature",
                  "default": 1
                },
                "max_tokens": {
                  "type": "integer",
                  "format": "int32",
                  "description": "One token equals roughly 4 characters of text (up to 4000 or more tokens between prompt and completion, depending on model)",
                  "title": "max tokens"
                },
                "top_p": {
                  "type": "number",
                  "format": "float",
                  "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
                  "title": "top p"
                },
                "frequency_penalty": {
                  "type": "number",
                  "format": "float",
                  "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the models likelihood to repeat the same line verbatim.",
                  "title": "frequency penalty",
                  "default": 0
                },
                "presence_penalty": {
                  "type": "number",
                  "format": "float",
                  "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the models likelihood to talk about new topics.",
                  "title": "presence penalty",
                  "default": 0
                },
                "stop": {
                  "type": "array",
                  "items": {
                    "type": "string"
                  },
                  "description": "Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence"
                }
              },
              "required": [
                "model",
                "messages"
              ]
            }
          }
        ]
      }
    },
    "/v1/completions": {
      "post": {
        "responses": {
          "200": {
            "description": "GPT3 completed your prompt",
            "schema": {
              "type": "object",
              "properties": {
                "id": {
                  "type": "string",
                  "description": "id"
                },
                "object": {
                  "type": "string",
                  "description": "object"
                },
                "created": {
                  "type": "integer",
                  "format": "int32",
                  "description": "created"
                },
                "model": {
                  "type": "string",
                  "description": "Which GPT3 Engine was used",
                  "title": "Model",
                  "x-ms-visibility": "internal"
                },
                "choices": {
                  "type": "array",
                  "items": {
                    "type": "object",
                    "properties": {
                      "text": {
                        "type": "string",
                        "description": "Completion text",
                        "title": "Text"
                      },
                      "index": {
                        "type": "integer",
                        "format": "int32",
                        "description": "Number of completion",
                        "title": "Index"
                      },
                      "finish_reason": {
                        "title": "Finish reason",
                        "type": "string",
                        "description": "Reason why the text finished (stop condition / natural end / length)"
                      },
                      "usage": {
                        "type": "object",
                        "properties": {
                          "prompt_tokens": {
                            "type": "integer",
                            "format": "int32",
                            "description": "Number of tokens in the prompt",
                            "title": "Prompt Tokens"
                          },
                          "completion_tokens": {
                            "type": "integer",
                            "format": "int32",
                            "description": "Number of tokens in the completion",
                            "title": "Completion Tokens"
                          },
                          "total_tokens": {
                            "type": "integer",
                            "format": "int32",
                            "description": "Total number of tokens in prompt and completion",
                            "title": "Total Tokens"
                          }
                        }
                      }
                    }
                  },
                  "description": "Returned Completion(s)"
                }
              }
            }
          }
        },
        "summary": "GPT3 Completes your prompt",
        "description": "GPT3 Completes your prompt",
        "operationId": "CompletionV2",
        "parameters": [
          {
            "name": "body",
            "in": "body",
            "required": true,
            "schema": {
              "type": "object",
              "properties": {
                "model": {
                  "type": "string",
                  "description": "The used model, choose between text-davinci-002, text-curie-001, text-babbage-001, text-ada-001",
                  "default": "text-davinci-002",
                  "x-ms-visibility": "important",
                  "x-ms-summary": "Engine",
                  "enum": [
                    "text-davinci-002",
                    "text-curie-001",
                    "text-babbage-001",
                    "text-ada-001"
                  ],
                  "x-ms-enum-values": [
                    {
                      "displayName": "DaVinci - most expensive",
                      "value": "text-davinci-002"
                    },
                    {
                      "displayName": "Curie",
                      "value": "text-curie-001"
                    },
                    {
                      "displayName": "Babbage",
                      "value": "text-babbage-001"
                    },
                    {
                      "displayName": "Ada - cheapest",
                      "value": "text-ada-001"
                    }
                  ]
                },
                "prompt": {
                  "type": "string",
                  "description": "Text that will be completed by GPT3",
                  "title": "prompt",
                  "default": "What is your favorite animal and why? Tell me also about the size and weight of this animal."
                },
                "n": {
                  "type": "integer",
                  "format": "int32",
                  "description": "How many completions to generate for each prompt",
                  "default": 1
                },
                "best_of": {
                  "type": "integer",
                  "format": "int32",
                  "description": "If set to more than 1, generates multiple completions server-side and returns the \"best\". Must be greater than \"n\". Use with caution, can consume a lot of tokens.",
                  "default": 1
                },
                "temperature": {
                  "type": "number",
                  "format": "float",
                  "description": "Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer. Use this OR top p",
                  "title": "temperature",
                  "default": 1
                },
                "max_tokens": {
                  "type": "integer",
                  "format": "int32",
                  "description": "One token equals roughly 4 characters of text (up to 4000 tokens between prompt and completion)",
                  "title": "max tokens",
                  "default": 100
                },
                "top_p": {
                  "type": "number",
                  "format": "float",
                  "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
                  "title": "top p"
                },
                "frequency_penalty": {
                  "type": "number",
                  "format": "float",
                  "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the models likelihood to repeat the same line verbatim.",
                  "title": "frequency penalty",
                  "default": 0
                },
                "presence_penalty": {
                  "type": "number",
                  "format": "float",
                  "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the models likelihood to talk about new topics.",
                  "title": "presence penalty",
                  "default": 0
                },
                "stop": {
                  "type": "array",
                  "items": {
                    "type": "string"
                  },
                  "description": "Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence"
                }
              },
              "required": [
                "model",
                "prompt"
              ]
            }
          }
        ],
        "x-ms-visibility": "important"
      }
    },
    "/v1/embeddings": {
      "post": {
        "responses": {
          "200": {
            "description": "default",
            "schema": {
              "type": "object",
              "properties": {
                "object": {
                  "type": "string",
                  "description": "object"
                },
                "data": {
                  "type": "array",
                  "items": {
                    "type": "object",
                    "properties": {
                      "object": {
                        "type": "string",
                        "description": "object"
                      },
                      "embedding": {
                        "type": "array",
                        "items": {
                          "type": "number",
                          "format": "float"
                        },
                        "description": "embedding"
                      },
                      "index": {
                        "type": "integer",
                        "format": "int32",
                        "description": "index"
                      }
                    }
                  },
                  "description": "data"
                },
                "model": {
                  "type": "string",
                  "description": "model"
                },
                "usage": {
                  "type": "object",
                  "properties": {
                    "prompt_tokens": {
                      "type": "integer",
                      "format": "int32",
                      "description": "prompt_tokens"
                    },
                    "total_tokens": {
                      "type": "integer",
                      "format": "int32",
                      "description": "total_tokens"
                    }
                  },
                  "description": "usage"
                }
              }
            }
          }
        },
        "summary": "Embeddings",
        "description": "Get a vector representation of a given input",
        "operationId": "Embeddings",
        "parameters": [
          {
            "name": "body",
            "in": "body",
            "required": false,
            "schema": {
              "type": "object",
              "properties": {
                "model": {
                  "type": "string",
                  "description": "model",
                  "title": "",
                  "default": "text-embedding-ada-002"
                },
                "input": {
                  "type": "string",
                  "description": "input",
                  "title": ""
                }
              },
              "required": [
                "input",
                "model"
              ]
            }
          }
        ]
      }
    },
    "/v1/engines/{engine}/completions": {
      "post": {
        "responses": {
          "200": {
            "description": "GPT3 completed your prompt",
            "schema": {
              "type": "object",
              "properties": {
                "id": {
                  "type": "string",
                  "description": "id"
                },
                "object": {
                  "type": "string",
                  "description": "object"
                },
                "created": {
                  "type": "integer",
                  "format": "int32",
                  "description": "created"
                },
                "model": {
                  "type": "string",
                  "description": "Which GPT3 Engine was used",
                  "title": "Model",
                  "x-ms-visibility": "internal"
                },
                "choices": {
                  "type": "array",
                  "items": {
                    "type": "object",
                    "properties": {
                      "text": {
                        "type": "string",
                        "description": "Completion text",
                        "title": "Text"
                      },
                      "index": {
                        "type": "integer",
                        "format": "int32",
                        "description": "Number of completion",
                        "title": "Index"
                      },
                      "logprobs": {
                        "type": "string",
                        "description": "Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 3, the API will return a list of the 3 most likely tokens.",
                        "title": "Logprobs"
                      },
                      "finish_reason": {
                        "title": "Finish reason",
                        "type": "string",
                        "description": "Reason why the text finished (stop condition / natural end / length)"
                      }
                    }
                  },
                  "description": "Returned Completion(s)"
                }
              }
            }
          }
        },
        "summary": "GPT3 Completes your prompt",
        "description": "GPT3 Completes your prompt (deprecated by OpenAI - use Completion_New)",
        "operationId": "Completion",
        "deprecated": true,
        "parameters": [
          {
            "name": "engine",
            "in": "path",
            "type": "string",
            "description": "The used engine, choose between text-davinci-002/003, text-curie-001, text-babbage-001, text-ada-001",
            "required": true,
            "default": "text-davinci-003",
            "x-ms-visibility": "important",
            "x-ms-summary": "Engine",
            "enum": [
              "text-davinci-003",
              "text-davinci-002",
              "text-curie-001",
              "text-babbage-001",
              "text-ada-001"
            ],
            "x-ms-enum-values": [
              {
                "displayName": "DaVinci (new)",
                "value": "text-davinci-003"
              },
              {
                "displayName": "DaVinci (old)",
                "value": "text-davinci-002"
              },
              {
                "displayName": "Curie",
                "value": "text-curie-001"
              },
              {
                "displayName": "Babbage",
                "value": "text-babbage-001"
              },
              {
                "displayName": "Ada",
                "value": "text-ada-001"
              }
            ]
          },
          {
            "name": "body",
            "in": "body",
            "required": true,
            "schema": {
              "type": "object",
              "properties": {
                "prompt": {
                  "type": "string",
                  "description": "Text that will be completed by GPT3",
                  "title": "prompt",
                  "default": "What is your favorite animal and why? Tell me also about the size and weight of this animal."
                },
                "n": {
                  "type": "integer",
                  "format": "int32",
                  "description": "How many completions to generate for each prompt",
                  "default": 1
                },
                "best_of": {
                  "type": "integer",
                  "format": "int32",
                  "description": "If set to more than 1, generates multiple completions server-side and returns the \"best\". Must be greater than \"n\". Use with caution, can consume a lot of tokens.",
                  "default": 1
                },
                "temperature": {
                  "type": "number",
                  "format": "float",
                  "description": "Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer. Use this OR top p",
                  "title": "temperature",
                  "default": 1
                },
                "max_tokens": {
                  "type": "integer",
                  "format": "int32",
                  "description": "One token equals roughly 4 characters of text (up to 4000 tokens between prompt and completion)",
                  "title": "max tokens",
                  "default": 100
                },
                "top_p": {
                  "type": "number",
                  "format": "float",
                  "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
                  "title": "top p"
                },
                "frequency_penalty": {
                  "type": "number",
                  "format": "float",
                  "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the models likelihood to repeat the same line verbatim.",
                  "title": "frequency penalty",
                  "default": 0
                },
                "presence_penalty": {
                  "type": "number",
                  "format": "float",
                  "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the models likelihood to talk about new topics.",
                  "title": "presence penalty",
                  "default": 0
                },
                "user": {
                  "type": "string",
                  "title": "user",
                  "description": "A unique identifier representing your end-user, which will help OpenAI to monitor and detect abuse"
                },
                "stop": {
                  "type": "array",
                  "items": {
                    "type": "string"
                  },
                  "description": "Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence"
                }
              },
              "required": [
                "prompt"
              ]
            }
          }
        ],
        "x-ms-visibility": "important"
      }
    },
    "/v1/images/generations": {
      "post": {
        "responses": {
          "200": {
            "description": "default",
            "schema": {
              "type": "object",
              "properties": {
                "created": {
                  "type": "integer",
                  "format": "int32",
                  "description": "Unique Creation ID from OpenAI",
                  "title": "created",
                  "x-ms-visibility": "internal"
                },
                "data": {
                  "type": "array",
                  "items": {
                    "type": "object",
                    "properties": {
                      "url": {
                        "type": "string",
                        "description": "URL to created Image",
                        "title": "url"
                      },
                      "b64_json": {
                        "type": "string",
                        "description": "Image in base64 format",
                        "title": "b64image",
                        "format": "byte"
                      }
                    }
                  },
                  "description": "data"
                }
              }
            }
          }
        },
        "summary": "Create an Image",
        "operationId": "CreateImage",
        "parameters": [
          {
            "name": "body",
            "in": "body",
            "required": false,
            "schema": {
              "type": "object",
              "properties": {
                "prompt": {
                  "type": "string",
                  "description": "The prompt that describes the image",
                  "title": "prompt",
                  "default": "An elephant with a fedora in a dark room, digital art"
                },
                "n": {
                  "type": "integer",
                  "format": "int32",
                  "description": "Number of images from 1 to 10",
                  "title": "Number of images",
                  "default": 1
                },
                "size": {
                  "type": "string",
                  "title": "size",
                  "description": "The size of the generated images. 256x256, 512x512 or 1024x1024 (default: 1024x1024)",
                  "enum": [
                    "256x256",
                    "512x512",
                    "1024x1024"
                  ],
                  "default": "1024x1024"
                },
                "response_format": {
                  "type": "string",
                  "title": "format",
                  "description": "Get url to picture or receive it in base64 format (default: url)",
                  "enum": [
                    "url",
                    "b64_json"
                  ],
                  "default": "url"
                }
              },
              "required": [
                "prompt"
              ]
            }
          }
        ],
        "description": "DallE2 creates an image from your prompt"
      }
    }
  },
  "definitions": {},
  "parameters": {},
  "responses": {},
  "securityDefinitions": {
    "API Key": {
      "type": "apiKey",
      "in": "header",
      "name": "Authorization"
    }
  },
  "security": [
    {
      "API Key": []
    }
  ],
  "tags": [],
  "x-ms-connector-metadata": [
    {
      "propertyName": "Website",
      "propertyValue": "https://openai.com/"
    },
    {
      "propertyName": "Privacy policy",
      "propertyValue": "https://openai.com/api/policies/terms/"
    },
    {
      "propertyName": "Categories",
      "propertyValue": "AI"
    }
  ]
}